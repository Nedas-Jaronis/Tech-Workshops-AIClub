{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f183bc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- .slide: data-background-color=\"#0f172a\" class=\"center\" -->\n",
    "\n",
    "# Prompt Engineering\n",
    "\n",
    "## From Prompts  \n",
    "## to Production AI Systems\n",
    "\n",
    "---\n",
    "\n",
    "**1 Hour Workshop**\n",
    "\n",
    "Design. Control. Optimize. Deploy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db4653",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Prompt Engineering?\n",
    "\n",
    "Prompt engineering is the process of designing inputs  \n",
    "to guide large language models toward:\n",
    "\n",
    "- Reliable outputs\n",
    "- Structured responses\n",
    "- Reduced hallucination\n",
    "- Improved reasoning\n",
    "\n",
    "---\n",
    "\n",
    "**LLMs predict tokens ‚Äî not truth.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191b82b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Prompt Engineering Matters\n",
    "\n",
    "Modern LLM systems like:\n",
    "\n",
    "- ChatGPT  \n",
    "- Claude  \n",
    "- Gemini  \n",
    "\n",
    "are probabilistic systems.\n",
    "\n",
    "---\n",
    "\n",
    "Same input  \n",
    "‚â†  \n",
    "Same output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1d5cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LLMs Are Not Deterministic\n",
    "\n",
    "Large Language Models:\n",
    "\n",
    "- Predict the next token using probability\n",
    "- Sample from distributions\n",
    "- Do not compute fixed answers\n",
    "\n",
    "---\n",
    "\n",
    "The same question  \n",
    "can produce  \n",
    "different outputs.\n",
    "\n",
    "---\n",
    "\n",
    "Let‚Äôs test this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d385e02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Live Test\n",
    "\n",
    "We will use the same prompt  \n",
    "across multiple models.\n",
    "\n",
    "Observe:\n",
    "- Differences in reasoning\n",
    "- Differences in formatting\n",
    "- Confidence level\n",
    "\n",
    "\n",
    "prompt = \"\"\"A bat and a ball cost $1.10 total. The bat costs $1 more than the ball. How much does the ball cost?\"\"\"\n",
    "\n",
    "print(\"Run this prompt in multiple models:\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2c1c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Temperature & Randomness\n",
    "\n",
    "Temperature controls  \n",
    "how the model samples probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "Higher temperature:\n",
    "\n",
    "- More creative\n",
    "- More diverse outputs\n",
    "- Less predictable\n",
    "\n",
    "---\n",
    "\n",
    "Lower temperature:\n",
    "\n",
    "- More deterministic\n",
    "- More repeatable\n",
    "- More stable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c35ebf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reasoning Models vs Traditional LLMs\n",
    "\n",
    "## Traditional LLMs\n",
    "\n",
    "- Fast\n",
    "- Pattern-based completion\n",
    "- May shortcut reasoning\n",
    "\n",
    "---\n",
    "\n",
    "## Reasoning-Optimized Models\n",
    "\n",
    "- Explicit step-by-step reasoning\n",
    "- Greater logical depth\n",
    "- Slower, but often more accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88c690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reasoning Test\n",
    "\n",
    "Solve step-by-step:\n",
    "\n",
    "\"If 5 machines take 5 minutes to make 5 widgets,  \n",
    "how long do 100 machines take to make 100 widgets?\"\n",
    "\n",
    "---\n",
    "\n",
    "Now try:\n",
    "\n",
    "\"Solve without explanation.\"\n",
    "\n",
    "---\n",
    "\n",
    "Observe:\n",
    "\n",
    "- Does the answer change?\n",
    "- Does confidence change?\n",
    "- Does reasoning depth change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0c9e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Context Windows\n",
    "\n",
    "**Context Window** = How much text the model can remember at once.\n",
    "\n",
    "---\n",
    "\n",
    "Bigger context window:\n",
    "\n",
    "- Handles longer documents  \n",
    "- Maintains more history  \n",
    "- **Does NOT mean smarter**\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è Context overflow ‚Üí loss of early information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d898e58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hallucination\n",
    "\n",
    "**Hallucination** = Confident fabrication of information\n",
    "\n",
    "---\n",
    "\n",
    "Why it happens:\n",
    "\n",
    "- Model fills statistical gaps  \n",
    "- Optimized for coherence, not truth  \n",
    "- No internal fact-checking\n",
    "\n",
    "---\n",
    "\n",
    "The model prefers **sounding right**  \n",
    "over admitting uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca556599",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prompt Versioning\n",
    "\n",
    "Treat prompts like **code**.\n",
    "\n",
    "---\n",
    "\n",
    "**v1:** \"Summarize this article.\"  \n",
    "**v2:** \"Summarize this article in 5 bullet points.\"  \n",
    "**v3:** \"Summarize this article in 5 executive-level bullet points under 50 words.\"\n",
    "\n",
    "---\n",
    "\n",
    "Track changes.  \n",
    "Measure improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffabfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A/B Testing Prompts\n",
    "\n",
    "Same input.  \n",
    "Different prompt versions.  \n",
    "\n",
    "Compare output quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0da49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prompt Chaining\n",
    "\n",
    "Instead of one giant prompt:\n",
    "\n",
    "1. Extract entities  \n",
    "2. Analyze sentiment  \n",
    "3. Generate report\n",
    "\n",
    "Output of one prompt ‚Üí Input of next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb17093",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zero-Shot vs Multi-Shot Prompting\n",
    "\n",
    "**Zero-shot:**  \n",
    "\"Translate this to French.\"\n",
    "\n",
    "**Multi-shot:**  \n",
    "Provide 2‚Äì3 examples first.\n",
    "\n",
    "---\n",
    "\n",
    "Few-shot prompting dramatically improves format consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57abfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multimodal Prompting\n",
    "\n",
    "Modern models can process:\n",
    "\n",
    "- Text  \n",
    "- Images  \n",
    "- Code  \n",
    "- Audio\n",
    "\n",
    "---\n",
    "\n",
    "GPT-4o is a **multimodal model** capable of understanding text, images, and code simultaneously.  \n",
    "This enables structured extraction, document understanding, and advanced reasoning across data types.\n",
    "\n",
    "---\n",
    "\n",
    "References:\n",
    "\n",
    "- [Springer Chapter on GPT-4o Multimodal Capabilities](https://link.springer.com/chapter/10.1007/978-3-031-92611-2_4)  \n",
    "- [OpenAI GPT-4o Documentation](https://developers.openai.com/api/docs/models/gpt-4o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58bb6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üü¶ CELL 21 ‚Äî Markdown (Closing Slide)\n",
    "\n",
    "```markdown\n",
    "# Key Takeaways\n",
    "\n",
    "- LLMs are probabilistic systems\n",
    "- Prompt structure controls behavior\n",
    "- Versioning and testing improve reliability\n",
    "- Advanced techniques enable production-grade AI systems\n",
    "\n",
    "Prompt engineering is system design.\n",
    "Not just better questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d09e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adversarial Prompting / Jailbreaking\n",
    "\n",
    "Techniques used to:\n",
    "\n",
    "- Bypass safeguards  \n",
    "- Override system instructions  \n",
    "- Inject malicious instructions\n",
    "\n",
    "---\n",
    "\n",
    "Production systems must guard against:\n",
    "\n",
    "- Prompt injection  \n",
    "- Role-play exploits  \n",
    "- Instruction override attacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b5c92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Claude in Production Systems\n",
    "\n",
    "Capabilities:\n",
    "\n",
    "- Large context window  \n",
    "- Strong reasoning  \n",
    "- Tool use\n",
    "\n",
    "---\n",
    "\n",
    "Key Features:\n",
    "\n",
    "- **Skills**: Predefined abilities to perform structured tasks  \n",
    "- **Artifacts**: Reusable outputs, templates, or tools for workflows\n",
    "\n",
    "---\n",
    "\n",
    "References:\n",
    "\n",
    "- [Claude Artifacts](https://claude.ai/artifacts)  \n",
    "- [Claude Agents & Tools / Skills Overview](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview)\n",
    "\n",
    "---\n",
    "\n",
    "Claude is often used for structured reasoning workflows and production AI pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de882cc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Structured Outputs with BAML\n",
    "\n",
    "BAML allows you to define **schemas for AI outputs**  \n",
    "\n",
    "Instead of parsing messy text,  \n",
    "you enforce **structured responses**.\n",
    "\n",
    "---\n",
    "\n",
    "Example / More Info:\n",
    "\n",
    "- [BoundaryML ‚Äî BAML](https://boundaryml.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0159874",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "class TradeSignal:\n",
    "    action: str\n",
    "    confidence: float\n",
    "    reasoning: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07393b80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key Takeaways\n",
    "\n",
    "- LLMs are probabilistic systems  \n",
    "- Prompt structure controls behavior  \n",
    "- Versioning and testing improve reliability  \n",
    "- Advanced techniques enable production-grade AI systems\n",
    "\n",
    "---\n",
    "\n",
    "**Prompt engineering is system design**  \n",
    "Not just better questions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
